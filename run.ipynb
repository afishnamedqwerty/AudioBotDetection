{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Preparation**\n",
    "Data Acquisition: Download the specified datasets, which include 117,985 generated audio clips from Zenodo, along with the LJSPEECH and JSUT datasets for reference data.\n",
    "\n",
    "Pre-processing: Convert audio files into a uniform format if necessary (e.g., 16-bit PCM wav files). Extract Mel spectrograms from these audio files as they are a common feature used for training vocoders and detecting differences in generated audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "def map_to_array(batch):\n",
    "    speech_array, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch\n",
    "\n",
    "def load_ljspeech_dataset():\n",
    "    \"\"\"\n",
    "    Load the LJ Speech dataset and prepare it for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "    - audios: Numpy array of audio waveforms.\n",
    "    - texts: List of corresponding normalized text transcriptions.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('huggingface:lj_speech/main', split='train')\n",
    "    dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n",
    "    audios = []\n",
    "    texts = []\n",
    "\n",
    "    for example in tfds.as_numpy(dataset):\n",
    "        audio = example['audio']\n",
    "        #audio = example['speech'].astype(np.float32) / 32768.0  # Normalize int16 to float32 range [-1, 1]\n",
    "        text = example['normalized_text']\n",
    "        audios.append(audio)\n",
    "        texts.append(text)\n",
    "\n",
    "    return audios, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builders\\huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "\"ls_speech\" is not listed in Hugging Face datasets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m audios, texts \u001b[38;5;241m=\u001b[39m \u001b[43mload_ljspeech_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mload_ljspeech_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ljspeech_dataset\u001b[39m():\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Load the LJ Speech dataset and prepare it for feature extraction.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    - texts: List of corresponding normalized text transcriptions.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuggingface:ls_speech/main\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(map_to_array, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     25\u001b[0m     audios \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:643\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    513\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m ):\n\u001b[0;32m    529\u001b[0m   \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m   dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    651\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:498\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[1;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    497\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(name, data_dir\u001b[38;5;241m=\u001b[39mdata_dir, try_gcs\u001b[38;5;241m=\u001b[39mtry_gcs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:192\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mnamespace:\n\u001b[0;32m    191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mnamespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m huggingface_dataset_builder\u001b[38;5;241m.\u001b[39mbuilder(\n\u001b[0;32m    193\u001b[0m         name\u001b[38;5;241m=\u001b[39mname\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)\n\u001b[0;32m    194\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    195\u001b[0m       visibility\u001b[38;5;241m.\u001b[39mDatasetType\u001b[38;5;241m.\u001b[39mCOMMUNITY_PUBLIC\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m    196\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m community\u001b[38;5;241m.\u001b[39mcommunity_register\u001b[38;5;241m.\u001b[39mhas_namespace(name\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    197\u001b[0m   ):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m community\u001b[38;5;241m.\u001b[39mcommunity_register\u001b[38;5;241m.\u001b[39mbuilder(name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builders\\huggingface_dataset_builder.py:448\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, config, **builder_kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuilder\u001b[39m(\n\u001b[0;32m    446\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, config: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs\n\u001b[0;32m    447\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m HuggingfaceDatasetBuilder:\n\u001b[1;32m--> 448\u001b[0m   hf_repo_id \u001b[38;5;241m=\u001b[39m \u001b[43m_from_tfds_to_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HuggingfaceDatasetBuilder(\n\u001b[0;32m    450\u001b[0m       hf_repo_id\u001b[38;5;241m=\u001b[39mhf_repo_id, hf_config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs\n\u001b[0;32m    451\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\numeraiKitchen\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builders\\huggingface_dataset_builder.py:164\u001b[0m, in \u001b[0;36m_from_tfds_to_hf\u001b[1;34m(tfds_name)\u001b[0m\n\u001b[0;32m    162\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m from_hf_to_tfds(hf_name) \u001b[38;5;241m==\u001b[39m tfds_name\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hf_name\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m registered\u001b[38;5;241m.\u001b[39mDatasetNotFoundError(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not listed in Hugging Face datasets.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m )\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: \"ls_speech\" is not listed in Hugging Face datasets."
     ]
    }
   ],
   "source": [
    "audios, texts = load_ljspeech_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**\n",
    "MFCC and LFCC Features: Extract Mel-frequency cepstral coefficients (MFCC) and linear-frequency cepstral coefficients (LFCC) from the audio files. These features are crucial for capturing the textural properties of the sound and will serve as the input for the GMM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from python_speech_features import logfbank  # Assuming use of python_speech_features for LFCC\n",
    "\n",
    "def extract_lfcc(audio, sr=16000, n_filters=26, n_lfcc=20):\n",
    "    \"\"\"\n",
    "    Extracts Linear Frequency Cepstral Coefficients (LFCC) from an audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio: The audio signal from which to extract features.\n",
    "    - sr: The sample rate of the audio signal.\n",
    "    - n_filters: The number of filters to use in the filterbank.\n",
    "    - n_lfcc: The number of LFCCs to extract.\n",
    "\n",
    "    Returns:\n",
    "    - lfcc_features: An array of LFCC features averaged across time.\n",
    "    \"\"\"\n",
    "    # Compute log filterbank energies.\n",
    "    logfbank_features = logfbank(audio, samplerate=sr, nfilt=n_filters)\n",
    "    \n",
    "    # Compute DCT to get LFCCs, keep first 'n_lfcc' coefficients\n",
    "    lfcc_features = np.fft.dct(logfbank_features, type=2, axis=1, norm='ortho')[:, :n_lfcc]\n",
    "    return np.mean(lfcc_features, axis=0)\n",
    "\n",
    "def extract_features(audios, sr=16000):\n",
    "    \"\"\"\n",
    "    Wrapper function to load an audio file, pre-process it, and extract relevant features.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_path: Path to the audio file.\n",
    "    - sr: Sample rate to use for loading the audio.\n",
    "\n",
    "    Returns:\n",
    "    - features: A numpy array containing extracted features.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(audios, sr=sr)\n",
    "    lfcc = extract_lfcc(audio, sr)\n",
    "    return lfcc\n",
    "\n",
    "def scale_features(features):\n",
    "    \"\"\"\n",
    "    Scales the features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - features: Numpy array of features to scale.\n",
    "\n",
    "    Returns:\n",
    "    - scaled_features: Scaled features.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features.reshape(-1, 1))\n",
    "    return scaled_features.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**\n",
    "Gaussian Mixture Model (GMM): Train two GMMs for each dataset - one for the real audio distribution (using the original LJSPEECH dataset) and one for the generated audio samples. This step involves:\n",
    "    Calculating the likelihood ratio for classifying samples.\n",
    "    Using MFCC and LFCC features as inputs.\n",
    "    \n",
    "RawNet2 Training (Optional): As an alternative to GMM, train a CNN-GRU hybrid model known as RawNet2 which directly extracts features from raw audio to create embeddings for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def train_gmm(features, n_components=16):\n",
    "    \"\"\"\n",
    "    Train a Gaussian Mixture Model (GMM) on the provided features.\n",
    "\n",
    "    Parameters:\n",
    "    - features: Feature matrix for training the GMM.\n",
    "    - n_components: Number of Gaussian components in the GMM.\n",
    "\n",
    "    Returns:\n",
    "    - gmm: Trained GMM object.\n",
    "    \"\"\"\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=200, random_state=0)\n",
    "    gmm.fit(features)\n",
    "    return gmm\n",
    "\n",
    "def preprocess_features(audios, sr=22050, n_lfcc=20):\n",
    "    \"\"\"\n",
    "    Preprocess audio data and extract LFCC features.\n",
    "\n",
    "    Parameters:\n",
    "    - audios: List of audio waveforms.\n",
    "    - sr: Sample rate of the audio data.\n",
    "    - n_lfcc: Number of LFCC coefficients to extract.\n",
    "\n",
    "    Returns:\n",
    "    - features: Numpy array of extracted LFCC features for all audio samples.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for audio in audios:\n",
    "        lfcc = extract_lfcc(audio, sr=sr, n_lfcc=n_lfcc)\n",
    "        features.append(lfcc)\n",
    "    return np.array(features)\n",
    "\n",
    "def compute_eer(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Compute the Equal Error Rate (EER).\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth binary labels.\n",
    "    - y_scores: Predicted scores or probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - eer: Equal Error Rate.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "def classify_samples(gmm_real, gmm_synthetic, features):\n",
    "    \"\"\"\n",
    "    Classify samples using trained GMMs based on the log-likelihood ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - gmm_real: GMM trained on real audio features.\n",
    "    - gmm_synthetic: GMM trained on synthetic audio features.\n",
    "    - features: Feature matrix of samples to classify.\n",
    "\n",
    "    Returns:\n",
    "    - scores: Log-likelihood ratio scores for the samples.\n",
    "    \"\"\"\n",
    "    log_likelihood_real = gmm_real.score_samples(features)\n",
    "    log_likelihood_synthetic = gmm_synthetic.score_samples(features)\n",
    "    scores = log_likelihood_real - log_likelihood_synthetic\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and split the dataset\n",
    "features = np.vstack((features_real, features_synthetic))\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train GMMs\n",
    "gmm_real = train_gmm(X_train[y_train == 0], n_components=16)\n",
    "gmm_synthetic = train_gmm(X_train[y_train == 1], n_components=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier Design**\n",
    "Binary Classification: Design classifiers to distinguish between human and AI-generated voices. Use the GMM-based approach to calculate the likelihood of a sample being real or synthetic.\n",
    "\n",
    "Evaluation Metrics: Utilize the Equal Error Rate (EER) as the primary metric for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify and evaluation\n",
    "scores = classify_samples(gmm_real, gmm_synthetic, X_test)\n",
    "eer = compute_eer(y_test, scores)\n",
    "print(f\"Equal Error Rate (EER): {eer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments and Evaluation**\n",
    "Generalization Tests: Evaluate the classifier's performance on unseen data, such as JSUT and TTS datasets, to assess its ability to generalize.\n",
    "\n",
    "Attribution Analysis: Implement attribution methods like BlurIG to understand which parts of the audio signal influence the prediction, focusing on specific features that distinguish between human and AI-generated voices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numeraiKitchen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
